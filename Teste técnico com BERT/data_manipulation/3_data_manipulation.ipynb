{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dbora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dbora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dbora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\dbora\\\\nltk_data\\\\tokenizers\\\\punkt')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "import nltk\n",
    "nltk.download() \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenização bem-sucedida: ['Este', 'é', 'um', 'teste', 'simples', 'para', 'verificar', 'se', 'o', 'punkt', 'está', 'funcionando', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Texto de exemplo para testar a tokenização\n",
    "test_text = \"Este é um teste simples para verificar se o punkt está funcionando.\"\n",
    "\n",
    "# Tentar tokenizar o texto\n",
    "try:\n",
    "    tokens = word_tokenize(test_text)\n",
    "    print(\"Tokenização bem-sucedida:\", tokens)\n",
    "except LookupError as e:\n",
    "    print(\"Erro de Lookup:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with data\n",
    "texts = [\n",
    "    \"O gato está no telhado.\",\n",
    "    \"A chuva cai sem parar.\",\n",
    "    \"Gosto de assistir filmes nos finais de semana.\",\n",
    "    \"Ele está estudando para as provas finais.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords and non-alphabetic characters and return a list of clean words\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords and non-alphabetic characters from the input text and returns a list of clean words.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words with stopwords and non-alphabetic characters removed.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('portuguese')) \n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in word_tokens if word.isalpha() and word not in stop_words] \n",
    "    return filtered_words\n",
    "    # stop_words = set(stopwords.words('portuguese')) \n",
    "    # word_tokens = word_tokenize(text.lower())\n",
    "    # filtered_words = [word for word in word_tokens if word.isalpha() and word not in stop_words] \n",
    "    # return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned texts:\n",
      "['gato telhado', 'chuva cai parar', 'gosto assistir filmes finais semana', 'estudando provas finais']\n"
     ]
    }
   ],
   "source": [
    "# Function to return a list of texts as str\n",
    "def clean_texts(texts):\n",
    "    \"\"\"\n",
    "    Cleans a list of texts by removing stopwords and non-alphabetic characters from each text.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list of texts to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of cleaned texts as strings.\n",
    "    \"\"\"\n",
    "    return [\" \".join(clean_text(text)) for text in texts]\n",
    "    #return [\" \".join(clean_text(text)) for text in texts]\n",
    "\n",
    "# Apply the function\n",
    "cleaned_texts = clean_texts(texts)\n",
    "#cleaned_texts = clean_texts(texts)\n",
    "\n",
    "print(\"Cleaned texts:\")\n",
    "print(cleaned_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns lemmatized words\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"\n",
    "    Lemmatizes a list of words.\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): A list of words to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of lemmatized words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized texts:\n",
      "['gato telhado', 'chuva cai parar', 'gosto assistir filmes finais semana', 'estudando provas finais']\n"
     ]
    }
   ],
   "source": [
    "# Function that applies lemmatization and then joins the lemmatized words back into a str\n",
    "def lemmatize_texts(texts):\n",
    "    \"\"\"\n",
    "    Lemmatizes a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list of texts to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of lemmatized texts.\n",
    "    \"\"\"\n",
    "    return [\" \".join(lemmatize_words(text.split())) for text in texts]\n",
    "\n",
    "# Apply the function\n",
    "lemmatized_texts = lemmatize_texts(cleaned_texts)\n",
    "\n",
    "print(\"Lemmatized texts:\")\n",
    "print(lemmatized_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_vectors, tfidf_vectorizer\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m tfidf_vectors, tfidf_vectorizer \u001b[38;5;241m=\u001b[39m create_tfidf_vectors(\u001b[43mlemmatized_texts\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Function that creates TF-IDF vectors\n",
    "def create_tfidf_vectors(texts):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF vectors from a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list of texts to be transformed into TF-IDF vectors.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the TF-IDF vectors and the TF-IDF vectorizer.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_vectors, tfidf_vectorizer\n",
    "\n",
    "# Apply the function\n",
    "tfidf_vectors, tfidf_vectorizer = create_tfidf_vectors(lemmatized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 1:\n",
      "gato: 0.7071067811865476\n",
      "telhado: 0.7071067811865476\n",
      "\n",
      "Text 2:\n",
      "cai: 0.5773502691896257\n",
      "chuva: 0.5773502691896257\n",
      "parar: 0.5773502691896257\n",
      "\n",
      "Text 3:\n",
      "assistir: 0.4651619335222394\n",
      "filmes: 0.4651619335222394\n",
      "finais: 0.3667390112974172\n",
      "gosto: 0.4651619335222394\n",
      "semana: 0.4651619335222394\n",
      "\n",
      "Text 4:\n",
      "estudando: 0.6176143709756019\n",
      "finais: 0.48693426407352264\n",
      "provas: 0.6176143709756019\n"
     ]
    }
   ],
   "source": [
    "# Store vectorized words\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# Convert the matrix to an array\n",
    "dense_matrix = tfidf_vectors.toarray()\n",
    "\n",
    "# Display terms and values\n",
    "for i, text in enumerate(lemmatized_texts): \n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    for j, value in enumerate(dense_matrix[i]):\n",
    "        if value > 0:\n",
    "            print(f\"{feature_names[j]}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates Bag of Words vectors\n",
    "def create_bow_vectors(texts):\n",
    "    \"\"\"\n",
    "    Creates Bag of Words (BoW) vectors from a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list of texts to be transformed into BoW vectors.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the BoW vectors and the BoW vectorizer.\n",
    "    \"\"\"\n",
    "    bow_vectorizer = CountVectorizer()\n",
    "    bow_vectors = bow_vectorizer.fit_transform(texts)\n",
    "    return bow_vectors, bow_vectorizer\n",
    "\n",
    "# Apply the function\n",
    "bow_vectors, bow_vectorizer = create_bow_vectors(lemmatized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['assistir' 'cai' 'chuva' 'estudando' 'filmes' 'finais' 'gato' 'gosto'\n",
      " 'parar' 'provas' 'semana' 'telhado']\n"
     ]
    }
   ],
   "source": [
    "# Display vectorized words\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense matrix:\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Display matrix\n",
    "dense_matrix = bow_vectors.toarray()\n",
    "print(\"Dense matrix:\")\n",
    "print(dense_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
